<!DOCTYPE html> <html lang="en"> <head> <title>TMCM / Best CodeLlama Copilot for M1 Mac</title> <meta name="author" content="TMCM"> <meta name="description" content="I’ve been experimenting with LLMs, particularly those with coding assistant capabilities such as CodeLlama from Meta. In this post, I’ll share the best open-source locally running options for a coding copilot-like assistant on a M1/M2 Mac.The language model I’ve been using is Phind-CodeLlama-34B-v2, specifically the GGUF quantised version from TheBloke (the “Q4_K_M” version has a good balance of speed and..."> <meta name="keywords" content="codellama, llama, copilot, AI"> <meta property="og:title" content="TMCM / Best CodeLlama Copilot for M1 Mac"> <meta property="og:description" content="I’ve been experimenting with LLMs, particularly those with coding assistant capabilities such as CodeLlama from Meta. In this post, I’ll share the best open-source locally running options for a coding copilot-like assistant on a M1/M2 Mac.The language model I’ve been using is Phind-CodeLlama-34B-v2, specifically the GGUF quantised version from TheBloke (the “Q4_K_M” version has a good balance of speed and..."> <meta property="og:image" content="/assets/images/blog/codellama-copilot.png"> <meta property="og:url" content="/best-codellama-copilot-m1-mac/"> <meta name="twitter:card" content="summary_large_image"> <meta property="og:site_name" content="TMCM"> <meta name="twitter:image:alt" content="TMCM"> <meta name="twitter:site" content="@tomasmcm"> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="robots" content="index,follow"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"> <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16"> <link rel="manifest" href="/manifest.json"> <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"> <meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="/assets/css/style.css?version=1694344798449714635"> <link rel="preload" as="font" href="/assets/fonts/now-regular-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-bold-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-light-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-medium-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Regular.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Medium.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Light.woff2" type="font/woff2" crossorigin="anonymous"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47256452-1"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-47256452-1");</script> </head> <body class="body--post"> <main class="main"> <header class="header"> <div class="container"> <h1 class="logo"> <a class="logo__link" href="/"> <span>T</span> <span>M</span> <span>C</span> <span>M</span> </a></h1> </div> </header> <header class="header--post" style="background-image: url('/assets/images/blog/codellama-copilot.png')"></header> <section class="post"> <div class="container"> <article class="article-wrapper"> <h1 class="h1">Best CodeLlama Copilot for M1 Mac</h1> <h5 class="h5 article-meta">Sep 10, 2023 · <span title="Estimated read time"> 6 mins read </span></h5> <p>I’ve been experimenting with LLMs, particularly those with coding assistant capabilities such as <a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">CodeLlama from Meta</a>. In this post, I’ll share the best open-source locally running options for a coding copilot-like assistant on a M1/M2 Mac.</p> <p>The language model I’ve been using is Phind-CodeLlama-34B-v2, specifically the <a href="https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF">GGUF quantised version from TheBloke</a> (the <em>“Q4_K_M”</em> version has a good balance of speed and quality). This is a fine-tuned version of the original CodeLlama 34B by Phind, and from my experiments it’s both faster to run inferences and yields better results compared to the standard model.</p> <h3 id="running-the-model-locally">Running the model locally</h3> <p>My preferred way of running the model locally has been through <a href="https://lmstudio.ai/">LM Studio</a>. This app allows downloading models directly from HuggingFace, configuring the parameters, chatting directly with a nice chat UI or running a local server with an OpenAI compatible API.</p> <p>Here is the config I’m using with Phind-CodeLlama-34B-v2:</p> <div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// ~/.cache/lm-studio/config-presets/phind_codellama.preset.json</span>
<span class="p">{</span>
  <span class="dl">"</span><span class="s2">name</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Phind CodeLlama v2</span><span class="dl">"</span><span class="p">,</span>
  <span class="dl">"</span><span class="s2">load_params</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
    <span class="dl">"</span><span class="s2">rope_freq_base</span><span class="dl">"</span><span class="p">:</span> <span class="mi">1000000</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">n_ctx</span><span class="dl">"</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span> <span class="c1">// Sets the max context to 4096 tokens</span>
    <span class="dl">"</span><span class="s2">n_gpu_layers</span><span class="dl">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1">// Enable inference using M1/M2 GPU</span>
    <span class="dl">"</span><span class="s2">use_mlock</span><span class="dl">"</span><span class="p">:</span> <span class="kc">false</span> <span class="c1">// Disable loading the model into RAM</span>
  <span class="p">},</span>
  <span class="dl">"</span><span class="s2">inference_params</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
    <span class="dl">"</span><span class="s2">input_prefix</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">### User Message</span><span class="dl">"</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">input_suffix</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">### Assistant</span><span class="dl">"</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">antiprompt</span><span class="dl">"</span><span class="p">:</span> <span class="p">[</span>
      <span class="dl">"</span><span class="s2">### User Message</span><span class="dl">"</span>
    <span class="p">],</span>
    <span class="dl">"</span><span class="s2">pre_prompt</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">### System Prompt</span><span class="se">\n</span><span class="s2">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span><span class="se">\n\n</span><span class="dl">"</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="vscode-integration">VSCode Integration</h3> <p>In order to get the best coding experience we need to have a way to query the model directly from the IDE we are working on. I’ll focus on VSCode as it’s the one I’ve been using.</p> <p>I found two good options for integrating a locally running LLM with VSCode: <a href="https://continue.dev/">continue.dev</a> and <a href="https://github.com/nvms/wingman">wingman</a>. Continue.dev is more similar to Copilot but requires a more complex setup, Wingman is simpler and the setup is more straightforward.</p> <h4 id="continuedev">Continue.dev</h4> <p>Continue is compatible with several model providers as described in their <a href="https://continue.dev/docs/customization">docs</a>. The reason it requires a more complex setup is because for every query you make it runs multiple inferences (e.g.: generate a title and summary for the query). For this reason, I’ve found the best setup is to only configure the CodeLlama model for the code inference and use another model provider for the rest. I’ve opted for TogetherAI as it has a generous trial credit and fast inference.</p> <p>This is the configuration I’m using with continue.dev:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ~/.continue/config.py
</span><span class="p">...</span>
<span class="kn">from</span> <span class="nn">continuedev.src.continuedev.libs.llm.together</span> <span class="kn">import</span> <span class="n">TogetherLLM</span>
<span class="kn">from</span> <span class="nn">continuedev.src.continuedev.libs.llm.ggml</span> <span class="kn">import</span> <span class="n">GGML</span>
<span class="p">...</span>
    <span class="n">models</span><span class="o">=</span><span class="n">Models</span><span class="p">(</span>
        <span class="n">small</span><span class="o">=</span><span class="n">TogetherLLM</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="s">"&lt;your-api-key&gt;"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s">"togethercomputer/llama-2-7b-chat"</span><span class="p">),</span>
        <span class="n">medium</span><span class="o">=</span><span class="n">TogetherLLM</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="s">"&lt;your-api-key&gt;"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s">"togethercomputer/llama-2-7b-chat"</span><span class="p">),</span>
        <span class="n">default</span><span class="o">=</span><span class="n">GGML</span><span class="p">(</span><span class="n">max_context_length</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">server_url</span><span class="o">=</span><span class="s">"http://localhost:8080"</span><span class="p">),</span>
        <span class="n">large</span><span class="o">=</span><span class="n">GGML</span><span class="p">(</span><span class="n">max_context_length</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">server_url</span><span class="o">=</span><span class="s">"http://localhost:8080"</span><span class="p">),</span>
        <span class="n">edit</span><span class="o">=</span><span class="n">GGML</span><span class="p">(</span><span class="n">max_context_length</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">server_url</span><span class="o">=</span><span class="s">"http://localhost:8080"</span><span class="p">),</span>
        <span class="n">chat</span><span class="o">=</span><span class="n">GGML</span><span class="p">(</span><span class="n">max_context_length</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">server_url</span><span class="o">=</span><span class="s">"http://localhost:8080"</span><span class="p">)</span>
    <span class="p">),</span>
<span class="p">...</span>
</code></pre></div></div> <p>With Continue you can also select some code in the editor to include it as context and then ask a question about it, which allows the model to offer better responses to your needs.</p> <h4 id="wingman">Wingman</h4> <p>Wingman offers a simpler yet very capable interface. Instead of a chat-based UI, there are multiple commands (with pre-built prompts) for specific tasks. Selecting a command runs the inference on the model and presents the response. Follow-up questions can be asked, and depending on the command selected you can include context from the current file too.</p> <p>Configuring wingman is quite simple, all you have to do is go to the extension settings and change:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Wingman <span class="o">&gt;</span> Openai: Api Base Url
<span class="s2">"http://localhost:8080/v1"</span>

Wingman <span class="o">&gt;</span> Openai: Model
<span class="s2">"Phind-CodeLlama-34B-v2-GGUF"</span>
</code></pre></div></div> <h3 id="conclusion">Conclusion</h3> <p>Using either method we are able to get a locally running code assistant with high quality output and quite decent inference times (if using a M1 Pro/Max chip with 32GB RAM). It is quite incredible what these models can achieve and the future seems very promising for even better results. It would be nicer to be able to run parallel inferences on two different models so that we would not need to rely on a third party provider for continue.dev, but that would also require more processing time and might increase the total inference time. Regardless, both projects are open source and quite stable during my time using them. Give them a try! <br/><br/></p> <p><small>Header image generated by BluePencil XL powered by Stable Diffusion XL AI.</small></p> </article> <div class="post-meta"> <p></p> </div> </div> </section> <section class="post-navigation"> <div class="container"> <a title="Previous Next.js Rendering Methods and their Pitfalls" href="/next-js-rendering-methods-and-their-pitfalls/" class="prev"> <span class="h2">Previous</span> <h2 class="h2">Next.js Rendering Methods and their Pitfalls</h2> </a> </div> </section> </main> <footer class="footer"> <div class="container footer__row"> <section class="footer__section"><h2 class="h2">IPFS</h2> <a title="IPFS Version" href="https://tomasmcm.eth.limo/" target="_blank">Decentralised version</a></section> <section class="footer__section"> <h2 class="h2">Follow</h2> <ul> <li><a title="Twitter" href="https://www.linkedin.com/in/tomasmcm/" target="_blank">Linkedin</a></li> <li><a title="Twitter" href="https://twitter.com/tomasmcm" target="_blank">Twitter</a></li> <li><a title="Github" href="https://github.com/tomasmcm" target="_blank">Github</a></li> </ul> </section> <section class="footer__section"> <h2 class="h2">More</h2> <ul> <li><a title="Archive" href="/archive/">Archive</a></li> <li><a title="Maker" href="/maker/">Maker</a></li> <li><a title="Blog" href="/blog/">Blog</a></li> </ul> </section> </div> </footer> </body> </html>