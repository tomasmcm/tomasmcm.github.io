<!DOCTYPE html> <html lang="en"> <head> <title>TMCM / Best CodeLlama Copilot for M1 Mac</title> <meta name="author" content="TMCM"> <meta name="description" content="I’ve been experimenting with LLMs, particularly those with coding assistant capabilities such as CodeLlama from Meta. In this post, I’ll share the best open-source locally running options for a coding copilot-like assistant on a M1/M2 Mac.The language model I’ve been using is Phind-CodeLlama-34B-v2, specifically the GGUF quantised version from TheBloke (the “Q4_K_M” version has a good balance of speed and..."> <meta name="keywords" content="codellama, llama, copilot, AI"> <meta property="og:title" content="TMCM / Best CodeLlama Copilot for M1 Mac"> <meta property="og:description" content="I’ve been experimenting with LLMs, particularly those with coding assistant capabilities such as CodeLlama from Meta. In this post, I’ll share the best open-source locally running options for a coding copilot-like assistant on a M1/M2 Mac.The language model I’ve been using is Phind-CodeLlama-34B-v2, specifically the GGUF quantised version from TheBloke (the “Q4_K_M” version has a good balance of speed and..."> <meta property="og:image" content="/assets/images/blog/codellama-copilot.png"> <meta property="og:url" content="/best-codellama-copilot-m1-mac/"> <meta name="twitter:card" content="summary_large_image"> <meta property="og:site_name" content="TMCM"> <meta name="twitter:image:alt" content="TMCM"> <meta name="twitter:site" content="@tomasmcm"> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="robots" content="index,follow"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"> <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16"> <link rel="manifest" href="/manifest.json"> <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"> <meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="/assets/css/style.css?version=1703594882636058763"> <link rel="preload" as="font" href="/assets/fonts/now-regular-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-bold-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-light-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-medium-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Regular.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Medium.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Light.woff2" type="font/woff2" crossorigin="anonymous"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47256452-1"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-47256452-1");</script> </head> <body class="body--post"> <main class="main"> <header class="header"> <div class="container"> <h1 class="logo"> <a class="logo__link" href="/"> <span>T</span> <span>M</span> <span>C</span> <span>M</span> </a></h1> </div> </header> <header class="header--post" style="background-image: url('/assets/images/blog/codellama-copilot.png')"></header> <section class="post"> <div class="container"> <article class="article-wrapper"> <h1 class="h1">Best CodeLlama Copilot for M1 Mac</h1> <h5 class="h5 article-meta">Sep 10, 2023 · <span title="Estimated read time"> 6 mins read </span></h5> <p>I’ve been experimenting with LLMs, particularly those with coding assistant capabilities such as <a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">CodeLlama from Meta</a>. In this post, I’ll share the best open-source locally running options for a coding copilot-like assistant on a M1/M2 Mac.</p> <p>The language model I’ve been using is Phind-CodeLlama-34B-v2, specifically the <a href="https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF">GGUF quantised version from TheBloke</a> (the <em>“Q4_K_M”</em> version has a good balance of speed and quality). This is a fine-tuned version of the original CodeLlama 34B by Phind, and from my experiments it’s both faster to run inferences and yields better results compared to the standard model.</p> <h3 id="running-the-model-locally">Running the model locally</h3> <p>My preferred way of running the model locally has been through <a href="https://lmstudio.ai/">LM Studio</a>. This app allows downloading models directly from HuggingFace, configuring the parameters, chatting directly with a nice chat UI or running a local server with an OpenAI compatible API.</p> <p>Here is the config I’m using with Phind-CodeLlama-34B-v2:</p> <div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// ~/.cache/lm-studio/config-presets/phind_codellama.preset.json</span>
<span class="p">{</span>
  <span class="dl">"</span><span class="s2">name</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Phind CodeLlama v2</span><span class="dl">"</span><span class="p">,</span>
  <span class="dl">"</span><span class="s2">load_params</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
    <span class="dl">"</span><span class="s2">rope_freq_base</span><span class="dl">"</span><span class="p">:</span> <span class="mi">1000000</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">n_ctx</span><span class="dl">"</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span> <span class="c1">// Sets the max context to 4096 tokens</span>
    <span class="dl">"</span><span class="s2">n_gpu_layers</span><span class="dl">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1">// Enable inference using M1/M2 GPU</span>
    <span class="dl">"</span><span class="s2">use_mlock</span><span class="dl">"</span><span class="p">:</span> <span class="kc">false</span> <span class="c1">// Disable loading the model into RAM</span>
  <span class="p">},</span>
  <span class="dl">"</span><span class="s2">inference_params</span><span class="dl">"</span><span class="p">:</span> <span class="p">{</span>
    <span class="dl">"</span><span class="s2">input_prefix</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">### User Message</span><span class="dl">"</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">input_suffix</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">### Assistant</span><span class="dl">"</span><span class="p">,</span>
    <span class="dl">"</span><span class="s2">antiprompt</span><span class="dl">"</span><span class="p">:</span> <span class="p">[</span>
      <span class="dl">"</span><span class="s2">### User Message</span><span class="dl">"</span>
    <span class="p">],</span>
    <span class="dl">"</span><span class="s2">pre_prompt</span><span class="dl">"</span><span class="p">:</span> <span class="dl">"</span><span class="s2">### System Prompt</span><span class="se">\n</span><span class="s2">Below is an instruction that describes a task. Write a response that appropriately completes the request.</span><span class="se">\n\n</span><span class="dl">"</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="vscode-integration">VSCode Integration</h3> <p>In order to get the best coding experience we need to have a way to query the model directly from the IDE we are working on. I’ll focus on VSCode as it’s the one I’ve been using.</p> <p>I found two good options for integrating a locally running LLM with VSCode: <a href="https://continue.dev/">continue.dev</a> and <a href="https://github.com/nvms/wingman">wingman</a>. Continue.dev is more similar to Copilot and offers a nice intuitive interface, Wingman is apparently simpler but provides a lot of pre-configured prompts.</p> <h4 id="continuedev">Continue.dev</h4> <p>Continue is compatible with several model providers as described in their <a href="https://continue.dev/docs/customization">docs</a>. I’ve found the default configuration to not be suitable for running a model locally, so I’ve tweaked it to suit my needs:</p> <ul> <li>disable telemetry to remove the requirement for internet connection,</li> <li>use <code class="language-plaintext highlighter-rouge">QueuedLLM</code> to ensure only one request is sent to the model at a time,</li> <li>use <code class="language-plaintext highlighter-rouge">GGML</code> to connect Continue to the OpenAI compatible API in LM Studio,</li> <li>disable summaries to reduce the number of requests made to the model.</li> </ul> <p>This is the configuration I’m using with continue.dev:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ~/.continue/config.py
</span><span class="p">...</span>
<span class="kn">from</span> <span class="nn">continuedev.src.continuedev.libs.llm.ggml</span> <span class="kn">import</span> <span class="n">GGML</span>
<span class="kn">from</span> <span class="nn">continuedev.src.continuedev.libs.llm.queued</span> <span class="kn">import</span> <span class="n">QueuedLLM</span>
<span class="p">...</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">ContinueConfig</span><span class="p">(</span>
    <span class="n">allow_anonymous_telemetry</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">models</span><span class="o">=</span><span class="n">Models</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="n">QueuedLLM</span><span class="p">(</span>
            <span class="n">llm</span><span class="o">=</span><span class="n">GGML</span><span class="p">(</span>
                <span class="n">context_length</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">server_url</span><span class="o">=</span><span class="s">"http://localhost:8080"</span>
            <span class="p">)</span>
        <span class="p">),</span>
    <span class="p">),</span>
    <span class="n">disable_summaries</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">...</span>
</code></pre></div></div> <p>With Continue you can also select some code in the editor to include it as context and then ask a question about it, which allows the model to offer better responses to your needs.</p> <h4 id="wingman">Wingman</h4> <p>Wingman offers a simpler yet very capable interface. Instead of a chat-based UI, there are multiple commands (with pre-built prompts) for specific tasks. Selecting a command runs the inference on the model and presents the response. Follow-up questions can be asked, and depending on the command selected you can include context from the current file too.</p> <p>Configuring wingman is quite simple, all you have to do is go to the extension settings and change:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Wingman <span class="o">&gt;</span> Openai: Api Base Url
<span class="s2">"http://localhost:8080/v1"</span>

Wingman <span class="o">&gt;</span> Openai: Model
<span class="s2">"Phind-CodeLlama-34B-v2-GGUF"</span>
</code></pre></div></div> <h3 id="conclusion">Conclusion</h3> <p>Using either method we are able to get a locally running code assistant with high quality output and quite decent inference times (if using a M1 Pro/Max chip with 32GB RAM). It is quite incredible what these models can achieve and the future seems very promising for even better results. It is amazing what we can achieve with free and/or open source projects and with great stability too! If you have a decent setup you can definitely use this instead of the commercial alternatives like Github Copilot, Amazon Code Whisperer or Tabnine. And with new fine-tuned models coming out every day this will only improve both in performance and better outputs. <br/><br/></p> <p><small>Header image generated by BluePencil XL powered by Stable Diffusion XL AI.</small> <br/></p> <p><small>Edit (2023-09-15): Removed TogetherLLM requirement by using the disable_summaries and QueuedLLM options in Continue.dev.</small></p> </article> <div class="post-meta"> <p></p> </div> </div> </section> <section class="post-navigation"> <div class="container"> <a title="Previous No-code your way to validation" href="/no-code-validation/" class="prev"> <span class="h2">Previous</span> <h2 class="h2">No-code your way to validation</h2> </a> <a title="Next The LLM Cycle" href="/the-llm-cycle/" class="next"> <span class="h2">Next</span> <h2 class="h2">The LLM Cycle</h2> </a> </div> </section> </main> <footer class="footer"> <div class="container footer__row"> <section class="footer__section"><h2 class="h2">IPFS</h2> <a title="IPFS Version" href="https://tomasmcm.eth.limo/" target="_blank">Decentralised version</a></section> <section class="footer__section"> <h2 class="h2">Follow</h2> <ul> <li><a title="Twitter" href="https://www.linkedin.com/in/tomasmcm/" target="_blank">Linkedin</a></li> <li><a title="Twitter" href="https://twitter.com/tomasmcm" target="_blank">Twitter</a></li> <li><a title="Github" href="https://github.com/tomasmcm" target="_blank">Github</a></li> </ul> </section> <section class="footer__section"> <h2 class="h2">More</h2> <ul> <li><a title="Archive" href="/archive/">Archive</a></li> <li><a title="Maker" href="/maker/">Maker</a></li> <li><a title="Blog" href="/blog/">Blog</a></li> </ul> </section> </div> </footer> </body> </html>