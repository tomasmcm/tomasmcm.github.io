<!DOCTYPE html> <html lang="en"> <head> <title>TMCM / The State of AI LLMs in Q2 2024</title> <meta name="author" content="TMCM"> <meta name="description" content="The artificial intelligence revolution is in full swing, with large language models (LLMs) leading the charge and creating new, transformative possibilities for startups and enterprises. As we navigate Q2 of 2024, the LLM landscape is vast and rapidly evolving, offering a wide array of models tailored to specific use cases, budgets and integration needs. From industry giants to emerging players..."> <meta name="keywords" content="AI, LLMs, GPT, Claude, Gemini, Llama"> <meta property="og:title" content="TMCM / The State of AI LLMs in Q2 2024"> <meta property="og:description" content="The artificial intelligence revolution is in full swing, with large language models (LLMs) leading the charge and creating new, transformative possibilities for startups and enterprises. As we navigate Q2 of 2024, the LLM landscape is vast and rapidly evolving, offering a wide array of models tailored to specific use cases, budgets and integration needs. From industry giants to emerging players..."> <meta property="og:image" content="/assets/images/blog/state-of-ai-llms-in-q2-2024-strategies.jpg"> <meta property="og:url" content="/state-of-ai-llms-in-q2-2024/"> <meta name="twitter:card" content="summary_large_image"> <meta property="og:site_name" content="TMCM"> <meta name="twitter:image:alt" content="TMCM"> <meta name="twitter:site" content="@tomasmcm"> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="robots" content="index,follow"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"> <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16"> <link rel="manifest" href="/manifest.json"> <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"> <meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="/assets/css/style.css?version=1718976501652910358"> <link rel="preload" as="font" href="/assets/fonts/now-regular-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-bold-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-light-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-medium-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Regular.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Medium.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Light.woff2" type="font/woff2" crossorigin="anonymous"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47256452-1"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-47256452-1");</script> </head> <body class="body--post"> <main class="main"> <header class="header"> <div class="container"> <h1 class="logo"> <a class="logo__link" href="/"> <span>T</span> <span>M</span> <span>C</span> <span>M</span> </a></h1> </div> </header> <header class="header--post" style="background-image: url('/assets/images/blog/state-of-ai-llms-in-q2-2024-strategies.jpg')"></header> <section class="post"> <div class="container"> <article class="article-wrapper"> <h1 class="h1">The State of AI LLMs in Q2 2024</h1> <h5 class="h5 article-meta">Jun 19, 2024 · <span title="Estimated read time"> 8 mins read </span></h5> <p>The artificial intelligence revolution is in full swing, with large language models (LLMs) leading the charge and creating new, transformative possibilities for startups and enterprises. As we navigate Q2 of 2024, the LLM landscape is vast and rapidly evolving, offering a wide array of models tailored to specific use cases, budgets and integration needs. From industry giants to emerging players and open-source options, selecting the right LLM can be a game-changer for automating processes, enhancing customer experiences, or opening up new business opportunities. This comprehensive guide explores the capabilities, strengths, and limitations of leading models, hosting options, and key considerations for making an informed decision. Let’s explore what’s on offer.</p> <h3 id="evaluating-the-top-llm-contenders">Evaluating the Top LLM Contenders</h3> <h4 id="openais-gpt-4-the-powerhouse-for-prompt-engineering-and-ai-assistants">OpenAI’s GPT-4: The Powerhouse for Prompt Engineering and AI Assistants</h4> <p>GPT-4 by OpenAI remains a powerhouse that excels in prompt engineering, role-playing scenarios, and specialised assistance tasks. Its ability to follow instructions via system prompts and provide structured output (JSON, XML) makes it a compelling choice for AI Assistant scenarios and AI-assisted workflow automations. However, its predecessor GPT-3.5, is becoming less attractive due to its pricing and output quality compared to newer models.</p> <h4 id="anthropics-claude-3-series-balancing-quality-and-cost-effectiveness">Anthropic’s Claude 3 Series: Balancing Quality and Cost-Effectiveness</h4> <p>Anthropic’s Claude 3 series has emerged as a strong contender. The Opus model delivers exceptional, human-like content generation, but it comes with a premium price tag (avg €41.3 per 1M tokens vs €19 for GPT-4 Turbo). From our experience, the Claude 3 Sonnet model offers a sweet spot – similar quality to GPT-4 at less than half the cost. Sonnet excels in content generation, making its way through follow-up messages, and can provide structured XML output when instructed properly. It’s worth noting that Sonnet only allows one system prompt and requires a strict sequence of human and assistant messages, creating some challenges when used in AI-workflows.</p> <p>If cost is a concern but quality is still of great importance, the Claude 3 Haiku model is a standout. It delivers impressive human-like content at an amazing price point and lightning-fast speeds. While it may not match Sonnet’s level in every scenario, Haiku shines in process automation tasks like summarization, key point extraction and triaging notes. Its 200k context window is a game-changer, and it works best when provided with enough examples of input and output, a kind of “on-demand fine-tuning” that allows much better results.</p> <h4 id="googles-gemini-models-massive-context-windows-enabling-new-use-cases">Google’s Gemini Models: Massive Context Windows enabling new Use Cases</h4> <p>Google’s Gemini models are worth considering too. The Gemini 1.0 Pro offers GPT-3.5-level quality with the added benefit of Google service integration. But the real star is the Gemini 1.5 Pro and Gemini 1.5 Flash, with its massive 1M (soon 2M) context window, enabling new use cases like retrieval-augmented generation (RAG) and style transfer using examples. Gemini models have recently added the option to provide system instructions (similar to a system prompt), enabling more use cases and user-facing applications. Early experiments show it is not as easy to shape into a persona as GPT-4 or Claude 3, but it may change as we learn more how to prompt it correctly.</p> <h4 id="coheres-command-model-grounded-and-sourced-outputs-for-factual-accuracy">Cohere’s Command Model: Grounded and Sourced Outputs for Factual Accuracy</h4> <p>For those seeking grounded and sourced outputs, Cohere’s Command model, trained specifically for retrieval-augmented generation (RAG) use cases, provides that out of the box. This model is particularly useful for scenarios where factual accuracy and transparency are paramount, as it can cite the sources it draws upon. While open-source versions of Command are available for local inference, they have minimum hardware requirements, which make the hosted options a better fit for most scenarios.</p> <p>It’s also worth noting that Cohere models are currently the only LLMs with straightforward API access and hosting that have native multi-language support. While GPT-4 and Claude 3 can speak many languages, the models were built with English in mind, so queries in languages that use different alphabets can cost 3 to 4 times more than the English equivalent. In contrast, Cohere Command only costs ~1.5 times more in the same languages.</p> <h4 id="open-source-models-for-local-inference-llama-3-and-fine-tuned-variants">Open-Source Models for Local Inference: Llama 3 and Fine-Tuned Variants</h4> <p>In the realm of open-source models for local inference, Llama 3 has emerged as a standout choice for private RAG tasks, such as reading over notes, summarising, and answering questions. As it is open-source, there are already fine-tuned versions offering up to a 4M context window. This large context window allows the model to process and understand more information, making it well suited for tasks that require in-depth analysis or synthesis of multiple sources.</p> <h3 id="the-quest-for-on-device-inference-balancing-size-speed-and-quality">The Quest for On-Device Inference: Balancing Size, Speed, and Quality</h3> <p>As the demand for on-device inference grows, several models are looking to strike the perfect balance between size, speed, quality, and low hardware requirements. Models like Phi-3-mini (3.8B), Gemma (2B), Qwen (1.8B and 0.5B), StableLM (1.6B), and Gemini Nano (1.8B and 3.25B) are all contenders in this space. While achieving this balance is no easy feat, the sweet spot seems to be around the 2B token range, as models of this size can run smoothly on devices with as little as 4GB of RAM.</p> <p>It’s worth noting that while smaller models may be more resource-efficient, they often sacrifice language understanding and knowledge capabilities. Striking the right balance between model size and performance is an ongoing challenge, and organisations must carefully weigh their specific needs and constraints when selecting a model for on-device inference.</p> <h3 id="hosting-options-for-llms">Hosting Options for LLMs</h3> <h4 id="major-cloud-providers-azure-ai-aws-bedrock-and-google-cloud-vertex-ai">Major Cloud Providers: Azure AI, AWS Bedrock, and Google Cloud Vertex AI</h4> <p>When it comes to hosting LLMs, organisations have a wide range of options from major cloud providers and specialty platforms. Besides the original model providers (OpenAI, Anthropic, and Mistral) which offer shared infrastructure hosting, it is now very easy to get a private instance of a model to ensure control over your data.</p> <p>Azure AI offers private access to OpenAI models such as GPT-3.5, GPT-4, Whisper, and DALL·E, as well as open-source models: Llama (v2 and v3), Mistral, and Cohere, all with token-based pricing. Additionally, Azure AI provides other models with provisioned hosting on a time-based pricing model. </p> <p>AWS Bedrock is another popular choice, offering private instances of Anthropic’s Claude models (Opus, Sonnet, Haiku), along with open-source options like Cohere, Mistral, and Llama, all with token-based pricing.</p> <p>Google Cloud Vertex AI is also in the picture, with access to Google’s Gemini models (Gemini 1.0 Pro and Gemini 1.5 Pro), Anthropic’s Claude 3 series (Opus, Sonnet, Haiku), and other models with provisioned, time-based hosting.</p> <h4 id="specialty-providers-groq-and-togetherai">Specialty Providers: Groq and Together.ai</h4> <p>In addition to the major cloud providers, specialty providers like Groq (<a href="https://wow.groq.com/#apply:~:text=the%20Developer%20Console.-,Price,-Groq%20guarantees%20to">wow.groq.com</a>) and Together.ai (<a href="https://together.ai/">together.ai</a>) have emerged to cater to specific needs. Groq, for instance, focuses on delivering high-speed, low-latency AI inference, which can be particularly valuable for time-sensitive or compute-intensive applications. Together.ai, on the other hand, specialises in hosting open-source models with token-based pricing, making it an attractive option for organisations looking to leverage the power of open-source LLMs while benefiting from a managed hosting solution.</p> <p>As the LLM landscape evolves super fast, startups and enterprises must carefully evaluate models to ensure innovation, streamline processes, and gain a competitive advantage. However, navigating AI’s complexities can be daunting without dedicated expertise. </p> </article> <div class="post-meta"> <span class="h2">Source</span> <p><a title="Whitesmith.co" href="https://www.whitesmith.co/blog/state-of-ai-llms-in-q2-2024-strategies/" target="_blank" rel="noopener">Whitesmith.co</a></p> </div> </div> </section> <section class="post-navigation"> <div class="container"> <a title="Previous Leveraging LLMs for Data Analysis" href="/leveraging-llms-for-data-analysis/" class="prev"> <span class="h2">Previous</span> <h2 class="h2">Leveraging LLMs for Data Analysis</h2> </a> </div> </section> </main> <footer class="footer"> <div class="container footer__row"> <section class="footer__section"><h2 class="h2">IPFS</h2> <a title="IPFS Version" href="https://tomasmcm.eth.limo/" target="_blank">Decentralised version</a></section> <section class="footer__section"> <h2 class="h2">Follow</h2> <ul> <li><a title="Twitter" href="https://www.linkedin.com/in/tomasmcm/" target="_blank">Linkedin</a></li> <li><a title="Twitter" href="https://twitter.com/tomasmcm" target="_blank">Twitter</a></li> <li><a title="Github" href="https://github.com/tomasmcm" target="_blank">Github</a></li> </ul> </section> <section class="footer__section"> <h2 class="h2">More</h2> <ul> <li><a title="Archive" href="/archive/">Archive</a></li> <li><a title="AI LLMs" href="/llms/">AI LLMs</a></li> <li><a title="Maker" href="/maker/">Maker</a></li> <li><a title="Blog" href="/blog/">Blog</a></li> </ul> </section> </div> </footer> </body> </html>