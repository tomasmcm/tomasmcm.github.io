<!DOCTYPE html> <html lang="en"> <head> <title>TMCM / Agentic R2A: A New Approach to Automated Document Analysis with LLMs</title> <meta name="author" content="TMCM"> <meta name="description" content="While working on a recent project, we faced a common challenge: analysing very long documents with Large Language Models (LLMs). Although modern LLMs can process impressive amounts of text (up to ~150K words), their ability to reason effectively deteriorates significantly when dealing with large quantities of information.We observed that when using more than half of the context window (typically above..."> <meta name="keywords" content="AI, LLMs, Agents, Workflows"> <meta property="og:title" content="TMCM / Agentic R2A: A New Approach to Automated Document Analysis with LLMs"> <meta property="og:description" content="While working on a recent project, we faced a common challenge: analysing very long documents with Large Language Models (LLMs). Although modern LLMs can process impressive amounts of text (up to ~150K words), their ability to reason effectively deteriorates significantly when dealing with large quantities of information.We observed that when using more than half of the context window (typically above..."> <meta property="og:image" content="/assets/images/blog/729ee852-f278-4977-ada9-49b8d28959d5.jpeg"> <meta property="og:url" content="/agentic-r2a/"> <meta name="twitter:card" content="summary_large_image"> <meta property="og:site_name" content="TMCM"> <meta name="twitter:image:alt" content="TMCM"> <meta name="twitter:site" content="@tomasmcm"> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="robots" content="index,follow"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"> <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16"> <link rel="manifest" href="/manifest.json"> <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"> <meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="/assets/css/style.css?version=1735394774192549107"> <link rel="preload" as="font" href="/assets/fonts/now-regular-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-bold-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-light-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-medium-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Regular.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Medium.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Light.woff2" type="font/woff2" crossorigin="anonymous"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47256452-1"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-47256452-1");</script> </head> <body class="body--post"> <main class="main"> <header class="header"> <div class="container"> <h1 class="logo"> <a class="logo__link" href="/"> <span>T</span> <span>M</span> <span>C</span> <span>M</span> </a></h1> </div> </header> <header class="header--post" style="background-image: url('/assets/images/blog/729ee852-f278-4977-ada9-49b8d28959d5.jpeg')"></header> <section class="post"> <div class="container"> <article class="article-wrapper"> <h1 class="h1">Agentic R2A: A New Approach to Automated Document Analysis with LLMs</h1> <h5 class="h5 article-meta">Nov 26, 2024 · <span title="Estimated read time"> 7 mins read </span></h5> <p>While working on a recent project, we faced a common challenge: analysing very long documents with Large Language Models (LLMs). Although modern LLMs can process impressive amounts of text (up to ~150K words), their ability to reason effectively deteriorates significantly when dealing with large quantities of information.</p> <p>We observed that when using more than half of the context window (typically above ~75K words), models tend to get confused, often forgetting or mixing up instructions. Additionally, processing these large documents led to significant practical challenges: we were frequently hitting rate limits in terms of tokens per minute, and the costs increased rapidly as each analysis required processing the entire document multiple times, even when using prompt caching techniques.</p> <p>This combination of technical limitations and practical constraints led us to develop a new approach: Agentic R2A (Agentic Retrieval-augmented Analysis). The Agentic nature of this approach refers to its ability to execute multi-step workflows where each step is an agent with a specific task and feeds data into subsequent steps, creating a chain of interconnected analyses.</p> <h3 id="beyond-traditional-rag">Beyond Traditional RAG</h3> <p>To understand Agentic R2A, let’s first look at traditional Retrieval Augmented Generation (RAG). RAG operates in two stages:</p> <ol> <li>Document Processing: Converting documents into chunks and embeddings, storing them in a vector store</li> <li>Query Processing: Generating embeddings for user questions, finding relevant chunks, and using them to inform the LLM’s response</li> </ol> <p>However, our use case was different. We needed to perform automated analysis of documents without user queries, following predetermined analysis steps. This realisation led us to develop Agentic R2A, which builds upon RAG’s foundations while optimising for automated workflows.</p> <h3 id="how-agentic-r2a-works">How Agentic R2A Works</h3> <p>Agentic R2A introduces a three-stage approach:</p> <ol> <li>Analysis Step Processing: <ul> <li>Extract main concepts from each analysis step prompt to be used in similarity search for document chunks</li> <li>Generate embeddings for these concepts</li> <li>Store them in a vector store</li> </ul> </li> <li>Document Processing: <ul> <li>Split documents into appropriately sized chunks</li> <li>Convert chunks into embeddings using models like Voyage, Cohere or OpenAI</li> <li>Store chunks in a vector store with metadata</li> </ul> </li> <li>Automated Analysis: <ul> <li>For each analysis step, retrieve the pre-computed concept embeddings for that step, and perform a similarity search against the document chunks in the vector store</li> <li>Run the Agent with a custom prompt specific to each analysis step and with the relevant chunks content</li> <li>Generate analysis output for each step and feed it to the next step in the workflow</li> </ul> </li> </ol> <p><img src="/assets/images/blog/r2a.png" alt="RAG vs Agentic R2A"/> <br/></p> <h3 id="key-differences-from-rag">Key Differences from RAG</h3> <p>While RAG is optimised for chat contexts, Agentic R2A is designed specifically for workflows. This fundamental difference leads to several advantages:</p> <ul> <li>No Runtime Embedding Generation: Unlike RAG, which needs to generate embeddings for each user query, Agentic R2A’s concept embeddings are generated once and reused for every workflow run.</li> <li>Workflow-Optimised Processing: Where RAG requires new prompt engineering for each user query, Agentic R2A operates on predefined workflows with carefully optimised prompts and analysis patterns that remain consistent across runs.</li> <li>Sequential Analysis Steps: Unlike RAG’s independent query-response pattern, Agentic R2A works with Agents connected through analysis steps, where each step has a dedicated prompt and feeds data into subsequent steps. This creates a sophisticated chain of analysis that builds upon previous insights.</li> <li>​​Efficient Resource Allocation: RAG splits computational resources between embedding generation and LLM processing at runtime. Agentic R2A focuses resources on similarity search and LLM processing, making it more efficient for batch processing operations.</li> <li>Predictable Scaling: Unlike RAG, which scales with the number of user queries, Agentic R2A’s scaling is determined by document size and number of analysis steps, making it more predictable for production environments.</li> </ul> <p>These fundamental differences make Agentic R2A particularly well-suited for automated document processing pipelines where consistency and efficiency are crucial.</p> <h3 id="applications">Applications</h3> <p>Agentic R2A has proven particularly effective in several use cases:</p> <p>Automating Online Course Creation:</p> <ul> <li>Process textbooks or educational materials as source documents</li> <li>Analysis steps focus on identifying key concepts, learning objectives, and examples</li> <li>Generate structured lesson plans by retrieving relevant chunks for each course module</li> </ul> <p>Generating Podcast or Video Scripts:</p> <ul> <li>Analyse long-form content like research papers or articles</li> <li>Extract main talking points and supporting evidence through targeted retrieval</li> <li>Structure content into natural conversation flows using specific concept embeddings</li> </ul> <p>Long Document Analysis:</p> <ul> <li>Process lengthy technical documentation or research papers</li> <li>Extract specific patterns, methodologies, or findings through targeted concept searches</li> <li>Create structured reports with consistent formatting and organisation</li> </ul> <p>In each case, Agentic R2A’s workflow-centric approach ensures consistent output quality while handling large volumes of source material efficiently. The pre-computed concept embeddings for each analysis step ensure that the system quickly identifies and processes the most relevant portions of the source documents.</p> <h3 id="implementation-considerations">Implementation Considerations</h3> <p>For optimal results, consider:</p> <ul> <li>Chunk sizing: Define minimum and maximum sizes for document chunks, balancing between context preservation and processing efficiency. If the document is divided into sections, aim to keep sections in the same chunk.</li> <li>Embedding quality: Choose appropriate embedding models for your domain, considering factors like semantic understanding, language support and cost. Different embedding models (Voyage, Cohere, OpenAI) have varying strengths in handling technical content, multiple languages or domain-specific terminology.</li> <li>Similarity thresholds: Adjust based on the specificity needed in analysis. These thresholds often need adjusting based on document types and analysis requirements.</li> <li>Dynamic chunk retrieval: Scale the number of chunks based on document size and complexity, ensuring comprehensive coverage without overwhelming the LLM’s context window.</li> <li>Prompt engineering: Each Agent should only perform a single well defined task and return a clear output. By combining multiple agents with different prompts we can perform complex analyses with consistent results.</li> <li>Document size: This technique only works well when the documents are larger than at least one third of the context window (and it becomes really useful for documents above half the context window). For smaller documents, adding the full doc content in the prompt is still the best approach as that provides all the required information for all steps.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Agentic R2A represents a significant step forward in automated document analysis. By optimising the RAG approach for autonomous workflows, we’ve created a more efficient and effective system for processing large documents with LLMs.</p> <p>The key innovation lies in its workflow-centric design, eliminating the need for runtime embedding generation while maintaining the benefits of retrieval-augmented approaches. As we continue to develop and refine this method, we expect to see even more applications where Agentic R2A can provide value in automated document analysis workflows.</p> </article> <div class="post-meta"> <span class="h2">Source</span> <p><a title="Whitesmith.co" href="https://www.whitesmith.co/blog/agentic-r2a/" target="_blank" rel="noopener">Whitesmith.co</a></p> </div> </div> </section> <section class="post-navigation"> <div class="container"> <a title="Previous The State of AI LLMs in Q2 2024" href="/state-of-ai-llms-in-q2-2024/" class="prev"> <span class="h2">Previous</span> <h2 class="h2">The State of AI LLMs in Q2 2024</h2> </a> <a title="Next LLMs in 2024: A Year of Integration and Innovation (And What’s Next)" href="/llms-in-2024/" class="next"> <span class="h2">Next</span> <h2 class="h2">LLMs in 2024: A Year of Integration and Innovation (And What’s Next)</h2> </a> </div> </section> </main> <footer class="footer"> <div class="container footer__row"> <section class="footer__section"><h2 class="h2">IPFS</h2> <a title="IPFS Version" href="https://tomasmcm.eth.limo/" target="_blank">Decentralised version</a></section> <section class="footer__section"> <h2 class="h2">Follow</h2> <ul> <li><a title="Twitter" href="https://www.linkedin.com/in/tomasmcm/" target="_blank">Linkedin</a></li> <li><a title="Twitter" href="https://twitter.com/tomasmcm" target="_blank">Twitter</a></li> <li><a title="Github" href="https://github.com/tomasmcm" target="_blank">Github</a></li> </ul> </section> <section class="footer__section"> <h2 class="h2">More</h2> <ul> <li><a title="Archive" href="/archive/">Archive</a></li> <li><a title="AI LLMs" href="/llms/">AI LLMs</a></li> <li><a title="Maker" href="/maker/">Maker</a></li> <li><a title="Blog" href="/blog/">Blog</a></li> </ul> </section> </div> </footer> </body> </html>