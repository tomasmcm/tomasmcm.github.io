<!DOCTYPE html> <html lang="en"> <head> <title>TMCM / To finetune or not to finetune</title> <meta name="author" content="TMCM"> <meta name="description" content="Teaching Chatbots new tricks: our learnings from Finetuning AI Models.We’ve been running some experiments recently with finetuning large language models (LLMs). Finetuning is when you further train an existing model on a new dataset to adapt its behaviour for a specific purpose.We explored different methods (axolotl, TinyLlama, llama.cpp) and different base models (“tinyllama-1.1b-chat”, “llama-2-7b”, “llama-160m”, “Flash-Llama-30M”, “TinyMistral-248M”) to see what..."> <meta name="keywords" content="AI, finetune, prompt-engineering"> <meta property="og:title" content="TMCM / To finetune or not to finetune"> <meta property="og:description" content="Teaching Chatbots new tricks: our learnings from Finetuning AI Models.We’ve been running some experiments recently with finetuning large language models (LLMs). Finetuning is when you further train an existing model on a new dataset to adapt its behaviour for a specific purpose.We explored different methods (axolotl, TinyLlama, llama.cpp) and different base models (“tinyllama-1.1b-chat”, “llama-2-7b”, “llama-160m”, “Flash-Llama-30M”, “TinyMistral-248M”) to see what..."> <meta property="og:image" content="/assets/images/blog/finetune.jpg"> <meta property="og:url" content="/to-finetune-or-not-to-finetune/"> <meta name="twitter:card" content="summary_large_image"> <meta property="og:site_name" content="TMCM"> <meta name="twitter:image:alt" content="TMCM"> <meta name="twitter:site" content="@tomasmcm"> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="robots" content="index,follow"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"> <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16"> <link rel="manifest" href="/manifest.json"> <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"> <meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="/assets/css/style.css?version=1703594882636058763"> <link rel="preload" as="font" href="/assets/fonts/now-regular-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-bold-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-light-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/now-medium-webfont.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Regular.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Medium.woff2" type="font/woff2" crossorigin="anonymous"> <link rel="preload" as="font" href="/assets/fonts/Radnika-Light.woff2" type="font/woff2" crossorigin="anonymous"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47256452-1"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-47256452-1");</script> </head> <body class="body--post"> <main class="main"> <header class="header"> <div class="container"> <h1 class="logo"> <a class="logo__link" href="/"> <span>T</span> <span>M</span> <span>C</span> <span>M</span> </a></h1> </div> </header> <header class="header--post" style="background-image: url('/assets/images/blog/finetune.jpg')"></header> <section class="post"> <div class="container"> <article class="article-wrapper"> <h1 class="h1">To finetune or not to finetune</h1> <h5 class="h5 article-meta">Dec 17, 2023 · <span title="Estimated read time"> 3 mins read </span></h5> <p>Teaching Chatbots new tricks: our learnings from Finetuning AI Models.</p> <p>We’ve been running some experiments recently with finetuning large language models (LLMs). Finetuning is when you further train an existing model on a new dataset to adapt its behaviour for a specific purpose.</p> <p>We explored different methods (axolotl, TinyLlama, llama.cpp) and different base models (“tinyllama-1.1b-chat”, “llama-2-7b”, “llama-160m”, “Flash-Llama-30M”, “TinyMistral-248M”) to see what works best. Here’s a quick summary of our key learnings:</p> <h4 id="chat-models-vs-text-models">Chat Models vs Text Models</h4> <p>Text models seem to learn better from a “completion” dataset <code class="language-plaintext highlighter-rouge">{"text":""}</code>. On chat models, I’m not sure if the finetuning is having an effect on the responses, as they probably require an “conversation” dataset <code class="language-plaintext highlighter-rouge">{"input":"","output": ""}</code> for training.</p> <h4 id="small-models-learn-faster">Small Models Learn Faster</h4> <p>Large models need more data and compute time to finetune compared to smaller ones. Large models are good for general chat, but small models adapt quicker for specialised use cases. The trade off is that small models tend to hallucinate more.</p> <h4 id="training-data-length-vs-expected-output">Training Data Length vs Expected Output</h4> <p>Using a large context takes significantly longer to train a model. I think that is only worth it if we are looking to also get long answers from the model. The training dataset items length should have an average length that is about the same as the expected context in future conversations. Items with context longer than the specified during training will be ignored past the limit.</p> <h4 id="hardware-considerations">Hardware Considerations</h4> <p>An M1 Max with a 10 core CPU is able to finetune a small model (up to 1b) with 2 epochs in a couple of hours. Bigger models would require CUDA GPUs to train in a reasonable time. Parallel/distributed computing is something that is being explored by some projects but requires an even more complex setup.</p> <h3 id="conclusion">Conclusion</h3> <p>Finetuning requires more data and resources than Prompt engineering, but can teach chatbots more subtle, intrinsic behaviours. For most use cases I think we should still use Prompt engineering. Finetuning is only relevant if there is a very specific way of speaking that cannot be requested using a prompt, or there is already a big (1k+) dataset of examples available.</p> <p>In summary, Finetuning is necessary when you want to change the intrinsic behaviours of a chatbot – the style, format and structure of responses, the use of specific vocabulary or figures of speech. Prompt engineering is still the best way to change what the model answers – up to date or private information. Prompt engineering can tell a model what to say, but Finetuning teaches it how to say it.</p> <h4 id="nerd-zone">Nerd zone</h4> <p>Here are 2 example commands we used to train a model using llama.cpp on an M1 Max:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># How to run finetuning on llama.cpp</span>
<span class="nv">$:</span> ../finetune <span class="se">\</span>
    <span class="nt">--model-base</span> ../models/Locutusque_TinyMistral-248M/ggml-model-f32.gguf <span class="se">\</span>
    <span class="nt">--train-data</span> ../train-data/dataset.txt <span class="se">\</span>
    <span class="nt">--threads</span> 10 <span class="se">\</span>
    <span class="nt">--batch</span> 4 <span class="se">\</span>
    <span class="nt">--sample-start</span> <span class="s2">"&lt;s&gt;"</span> <span class="se">\</span>
    <span class="nt">--ctx</span> 512 <span class="se">\</span>
    <span class="nt">--epochs</span> 4 <span class="se">\</span>
    <span class="nt">--checkpoint-in</span> ./checkpoint-LATEST.gguf
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># How to run inference on a finetuned model with llama.cpp</span>
<span class="nv">$:</span> ../main <span class="se">\</span>
    <span class="nt">--model</span> ../models/Locutusque_TinyMistral-248M/ggml-model-f32.gguf <span class="se">\</span>
    <span class="nt">--lora</span> ./TinyMistral-248M/ggml-lora-LATEST-f32.gguf <span class="se">\</span>
    <span class="nt">--temp</span> 0 <span class="se">\</span>
    <span class="nt">--prompt</span> <span class="s2">"An FCTO is not just a part-time Chief Technology Officer"</span>
</code></pre></div></div> </article> <div class="post-meta"> <p></p> </div> </div> </section> <section class="post-navigation"> <div class="container"> <a title="Previous GPTs And What They Mean For Companies Building With AI" href="/gpts-are-coming/" class="prev"> <span class="h2">Previous</span> <h2 class="h2">GPTs And What They Mean For Companies Building With AI</h2> </a> </div> </section> </main> <footer class="footer"> <div class="container footer__row"> <section class="footer__section"><h2 class="h2">IPFS</h2> <a title="IPFS Version" href="https://tomasmcm.eth.limo/" target="_blank">Decentralised version</a></section> <section class="footer__section"> <h2 class="h2">Follow</h2> <ul> <li><a title="Twitter" href="https://www.linkedin.com/in/tomasmcm/" target="_blank">Linkedin</a></li> <li><a title="Twitter" href="https://twitter.com/tomasmcm" target="_blank">Twitter</a></li> <li><a title="Github" href="https://github.com/tomasmcm" target="_blank">Github</a></li> </ul> </section> <section class="footer__section"> <h2 class="h2">More</h2> <ul> <li><a title="Archive" href="/archive/">Archive</a></li> <li><a title="Maker" href="/maker/">Maker</a></li> <li><a title="Blog" href="/blog/">Blog</a></li> </ul> </section> </div> </footer> </body> </html>